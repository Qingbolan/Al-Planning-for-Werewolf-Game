# 狼人杀AI项目概述

## 1. 项目背景

本项目基于"一夜狼人杀"（One Night Werewolf）游戏变体，开发一个基于强化学习的AI系统。该游戏具有以下特点：

- 白天阶段：包含三轮顺序发言，每轮发言更新发言历史
- 夜晚投票：单轮投票，具有反转的胜利条件（好人阵营投票导致狼人阵营获胜，反之亦然）
- 部分可观察性：玩家只能看到自己的身份，需要通过推理获取其他玩家的信息
- 多智能体交互：多个玩家之间的互动和博弈

## 2. 系统架构

### 2.1 当前代码结构

```
├── werewolf_env/          # 游戏环境
│   └── state.py           # 游戏状态管理（GameState类）
├── agents/                # 智能体实现
│   └── base_agent.py      # 基础智能体类（BaseAgent, RandomAgent, HeuristicAgent）
├── models/                # 模型定义
│   └── rl_agent.py        # 神经网络模型（WerewolfNetwork类和RLAgent实现）
├── train/                 # 训练相关
│   ├── base/              # 基础训练器
│   │   └── base_trainer.py # 基础训练器（BaseTrainer抽象类）
│   ├── multi_stage/       # 多阶段训练方案
│   │   ├── multi_stage_trainer.py # 多阶段训练器主类
│   │   ├── stage1_trainer.py      # 第一阶段训练器（启发式引导训练）
│   │   ├── stage2_trainer.py      # 第二阶段训练器（混合训练）
│   │   └── stage3_trainer.py      # 第三阶段训练器（自对弈训练）
├── utils/                 # 工具类
│   ├── belief_updater.py  # 信念状态更新（BeliefState和RoleSpecificBeliefUpdater类）
│   └── visualizer.py      # 可视化工具（BeliefVisualizer类）
└── documents/             # 文档
    ├── training_schemes/  # 训练方案文档
    │   └── multi_stage_training.md # 多阶段训练方案文档
    └── project_overview.md # 项目概述文档
```

### 2.2 核心组件

#### 2.2.1 游戏环境

- 状态管理：通过GameState类维护全局状态 s = ⟨R, H, φ, τ⟩，其中R表示真实玩家角色，H记录历史动作，φ表示阶段，τ表示轮数
- 游戏逻辑：实现游戏规则、行动验证、胜负判定等
- 观察空间：为每个玩家提供部分可观察的游戏状态

#### 2.2.2 智能体系统

- 基础智能体（BaseAgent）：定义通用接口和行为
- RL智能体（RLAgent）：基于PPO算法的Actor-Critic架构实现
- 启发式智能体（HeuristicAgent）：基于规则的决策系统

#### 2.2.3 训练系统

- 多阶段训练（MultiStageTrainer）：实现分阶段训练流程
  - 第一阶段（Stage1Trainer）：启发式引导训练
  - 第二阶段（Stage2Trainer）：混合训练
  - 第三阶段（Stage3Trainer）：自对弈训练
- 评估系统：定期评估模型性能
- 可视化工具：通过BeliefVisualizer监控训练过程和模型表现

### 2.3 信念系统

项目实现了完整的信念状态管理系统：

- BeliefState类：表示并维护玩家对其他角色的信念分布
- RoleSpecificBeliefUpdater类：根据不同角色的特性更新信念状态
- 可视化功能：通过BeliefVisualizer生成信念状态的可视化报告

## 3. 模型设计

### 3.1 状态表示

#### 3.1.1 观察空间

- 玩家信息：身份、状态、发言等
- 游戏阶段：当前阶段、轮数等
- 历史信息：投票记录、行动历史等
- 信念状态：通过BeliefState类维护对其他玩家身份的信念分布

#### 3.1.2 动作空间

- 发言动作：基于模板的有限状态空间 S_speech = ⟨S_type, S_target, S_content⟩
- 投票动作：选择投票对象
- 特殊能力：使用角色特殊能力

### 3.2 神经网络架构

#### 3.2.1 WerewolfNetwork模型

当前模型实现包括：

- 输入层：处理观察空间和信念状态
- 隐藏层：多层全连接网络
- 输出层：
  - 策略头：输出动作概率分布 π_θ(a|o,r,τ)
  - 价值头：评估当前状态价值

#### 3.2.2 信念更新机制

- 显式信念更新：基于角色信息、观察和行动更新信念
- 贝叶斯推理：根据新证据更新先验信念
- 规范化：确保信念分布总和为1

### 3.3 训练系统设计

#### 3.3.1 多阶段训练框架

当前实现的三阶段训练流程：

1. 启发式引导训练：RL智能体与启发式智能体对抗学习
2. 混合训练：RL智能体与预训练模型对抗学习
3. 自对弈训练：所有智能体使用同一个不断更新的模型

#### 3.3.2 奖励设计

- 胜利奖励：根据游戏胜负给予奖励
- 中间奖励：鼓励有益的中间行为
- 反转奖励：夜晚投票阶段的反转胜利条件

#### 3.3.3 优化目标

- 策略优化：最大化期望累积奖励 max_θ E[∑γ^t R(s_t,a_t,s_{t+1})]
- 价值优化：减小预测价值与实际回报的差距
- 探索与利用：通过PPO算法的熵正则化项平衡探索和利用

## 4. 关键技术实现

### 4.1 部分可观察马尔可夫决策过程(POMDP)

- 信念状态：通过BeliefState类实现对隐藏状态的信念分布
- 信念更新：通过RoleSpecificBeliefUpdater类实现基于观察和行动的信念更新
- 策略优化：基于信念状态的决策优化

### 4.2 多智能体强化学习

- 独立训练：在多阶段训练的不同阶段实现不同的训练策略
- 经验收集：收集多智能体交互的经验数据
- 自对弈：第三阶段实现完整的自对弈机制

### 4.3 深度强化学习

- PPO算法：处理高维状态空间和稳定训练
- 经验回放：重用历史经验提高样本效率
- 目标网络：在第三阶段训练器中实现目标网络更新机制

## 5. 项目改进与发展

### 5.1 代码重构

- 统一训练器接口：确保所有训练器遵循相同的基类接口
- 优化目录结构：将旧版训练器迁移到新的结构中
- 增强模块化：提高组件间的解耦和复用性

### 5.2 模型增强

- 改进网络架构：添加注意力机制和记忆模块
- 优化信念更新：提高信念更新的准确性和效率
- 扩展模板系统：丰富发言模板和策略选择

### 5.3 训练系统优化

- 完善多阶段训练：优化各阶段训练参数和衔接
- 添加课程学习：渐进增加任务难度
- 增强评估指标：多维度评估模型性能
