# 狼人杀奖励系统设计

## 概述

奖励系统是强化学习代理训练的核心组成部分，良好的奖励设计能够引导代理学习更有效的策略。本文档介绍狼人杀游戏中的奖励系统设计，包括基础奖励、阶段性奖励和角色特定奖励。

## 奖励系统架构

奖励系统实现在 `werewolf_env/rewards.py` 文件中的 `RewardFunction` 类，该类负责计算和跟踪玩家在游戏中的奖励值。每个奖励都有相应的权重，可以通过配置文件进行调整。

### 核心组件

1. **基础奖励计算**：针对动作成功/失败和游戏结果的基础奖励
2. **阶段性奖励计算**：针对游戏不同阶段（夜晚、讨论、投票）的特定奖励
3. **角色特定奖励计算**：针对不同角色（如狼人、预言家、强盗）的特定奖励
4. **累计奖励跟踪**：跟踪每个玩家的累计奖励

## 奖励类型

### 1. 基础奖励

基础奖励是所有角色共享的基本奖励信号，包括：

| 奖励类型 | 默认值 | 描述 |
|---------|-------|------|
| 胜利奖励 | +1.0 | 游戏结束时，如果玩家所在阵营获胜 |
| 失败惩罚 | -0.5 | 游戏结束时，如果玩家所在阵营失败 |
| 平局奖励 | 0.0 | 游戏结束时，如果游戏结果为平局 |
| 动作成功 | +0.1 | 玩家的动作执行成功 |
| 动作失败 | -0.05 | 玩家的动作执行失败 |

### 2. 阶段性奖励

阶段性奖励根据游戏的不同阶段给予特定奖励：

| 奖励类型 | 默认值 | 描述 |
|---------|-------|------|
| 正确指认 | +0.2 | 村民正确投票狼人 |
| 错误指认 | -0.1 | 村民错误投票其他村民 |
| 狼人误导 | +0.3 | 狼人成功误导村民投票村民 |
| 存活奖励 | +0.05 | 每轮游戏存活（从第二轮开始） |

### 3. 角色特定奖励

针对不同角色的特定奖励：

| 角色 | 奖励类型 | 默认值 | 描述 |
|------|---------|-------|------|
| 狼人 | 成功击杀 | +0.2 | 狼人成功击杀一名玩家 |
| 预言家 | 正确查验 | +0.2 | 预言家成功查验并发现狼人 |
| 强盗 | 战略性偷窃 | +0.15 | 强盗偷取特殊角色（非普通村民） |

## 奖励计算流程

奖励计算遵循以下流程：

1. **动作奖励计算**：首先计算基础的动作成功/失败奖励
2. **游戏结束检查**：如果游戏结束，计算游戏结果奖励
3. **阶段性奖励计算**：根据当前游戏阶段计算相应奖励
4. **角色特定奖励计算**：根据玩家角色计算特定奖励
5. **累计奖励更新**：更新玩家的累计奖励值

## 奖励系统配置

奖励系统可以通过配置文件进行自定义，修改不同奖励的权重：

```python
reward_config = {
    'win_reward': 1.0,
    'lose_reward': -0.5,
    'action_success_reward': 0.1,
    'action_failure_reward': -0.05,
    'correct_accusation_reward': 0.2,
    'incorrect_accusation_reward': -0.1,
    'werewolf_misdirection_reward': 0.3,
    'survival_per_round': 0.05,
    'werewolf_team_kill': 0.2,
    'seer_correct_check': 0.2,
    'robber_strategic_steal': 0.15,
}

# 创建奖励函数
reward_function = RewardFunction(config=reward_config)
```

## 待实现功能

当前奖励系统仍有一些待实现的功能：

1. **发言内容奖励**：基于NLP分析玩家发言内容给予奖励，如正确指认、成功误导等
2. **团队协作奖励**：狼人团队协作策略的奖励
3. **长期策略奖励**：鼓励代理考虑长期策略而非短期收益
4. **进阶调节机制**：根据玩家表现动态调整奖励权重

## 如何扩展

要扩展奖励系统，可以通过以下方式：

1. 在 `_setup_reward_weights` 方法中添加新的奖励权重
2. 在相应的计算方法中实现新奖励的计算逻辑
3. 在 `compute_reward` 方法中集成新的奖励计算

## 与训练系统的集成

奖励系统与多阶段训练系统集成，为不同训练阶段提供适当的奖励信号：

1. **第一阶段**（启发式引导训练）：更强调基础奖励和角色特定奖励
2. **第二阶段**（混合训练）：更强调团队协作和策略性奖励
3. **第三阶段**（自对弈训练）：强调综合性奖励和长期策略

## 参考资料

- [强化学习奖励设计最佳实践](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)
- [部分可观察马尔可夫决策过程中的奖励塑造](https://arxiv.org/abs/1909.09574)
- [多智能体强化学习中的奖励函数设计](https://arxiv.org/abs/1903.03661) 